{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76872ac7",
   "metadata": {},
   "source": [
    "# Choosing the Right Metric ‚Äì Real-World Scenarios\n",
    "\n",
    "In this notebook, you'll review real AI use cases and select the most appropriate evaluation metric for each one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ab817",
   "metadata": {},
   "source": [
    "### üè• Scenario 1 ‚Äì Medical Diagnosis (Cancer Detection)\n",
    "\n",
    "You're building a model to detect cancer from patient data. A **false negative** could mean missing a cancer case.\n",
    "\n",
    "**Q: Which metric is most critical? Why?**\n",
    "- A. Accuracy\n",
    "- B. Precision\n",
    "- C. Recall\n",
    "- D. F1 Score\n",
    "\n",
    "‚û°Ô∏è **Expected Answer**: **C. Recall** ‚Äì We want to minimize false negatives (catch all real positives).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf52c84",
   "metadata": {},
   "source": [
    "### üì© Scenario 2 ‚Äì Spam Email Detection\n",
    "\n",
    "Your model flags emails as spam. A **false positive** means marking a good email as spam.\n",
    "\n",
    "**Q: What metric matters most here?**\n",
    "- A. Accuracy\n",
    "- B. Precision\n",
    "- C. Recall\n",
    "- D. F1 Score\n",
    "\n",
    "‚û°Ô∏è **Expected Answer**: **B. Precision** ‚Äì Better to avoid flagging legitimate emails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6f2e3",
   "metadata": {},
   "source": [
    "### üí≥ Scenario 3 ‚Äì Credit Card Fraud Detection\n",
    "\n",
    "You're detecting fraudulent transactions. You want to balance false alarms with catching fraud.\n",
    "\n",
    "**Q: Best overall metric to track?**\n",
    "- A. Accuracy\n",
    "- B. Recall\n",
    "- C. F1 Score\n",
    "- D. AUC-ROC\n",
    "\n",
    "‚û°Ô∏è **Expected Answer**: **C. F1 Score** or **D. AUC-ROC** ‚Äì We need a balance and good threshold evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d1c8f",
   "metadata": {},
   "source": [
    "### üé¨ Scenario 4 ‚Äì Recommender System\n",
    "\n",
    "You‚Äôre evaluating how well a movie recommendation engine performs on ranking items.\n",
    "\n",
    "**Q: Which metric makes the most sense?**\n",
    "- A. Accuracy\n",
    "- B. Precision@k / MAP\n",
    "- C. F1 Score\n",
    "- D. ROC-AUC\n",
    "\n",
    "‚û°Ô∏è **Expected Answer**: **B. Precision@k / MAP** ‚Äì Ranking-based metrics are better for recommender systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92adda1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "Choosing the right metric depends on:\n",
    "- The **domain**\n",
    "- The **type of risk**\n",
    "- The **error type you want to avoid**\n",
    "\n",
    "Use this thinking on your CT-AI exam when evaluating model fitness.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
